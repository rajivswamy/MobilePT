{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rep Count Images Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinity AI Sample Data Processing\n",
    "Use images from Infinity AI Fitness Data Set to get terminal states of 3 exercises: squat, leg raise, and arm raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Videos data path and images output paths\n",
    "videos_in_path = \"./infinity_sample_data/\"\n",
    "images_out_folder = \"./rep_count_images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pose classes from video input folder\n",
    "pose_classes = sorted(n for n in os.listdir(videos_in_path) if not n.startswith(\".\"))\n",
    "\n",
    "states = [\"first\", \"second\"]\n",
    "\n",
    "for pose_class in pose_classes:\n",
    "    # Make subfolders for terminal state output, assume each exercise has \n",
    "    # two terminal states\n",
    "    for state in states:\n",
    "        if not os.path.exists(os.path.join(images_out_folder, pose_class + \"_\" + state)):\n",
    "            os.makedirs(os.path.join(images_out_folder, pose_class + \"_\" + state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of capture on sample video and json data input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirmed can acccess the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sample JSON and video from squat folder\n",
    "data_path = \"./infinity_sample_data/SQUAT/000000.json\"\n",
    "vid_path = \"./infinity_sample_data/SQUAT/000000.mp4\"\n",
    "file = open(data_path)\n",
    "vid_data = json.load(file)\n",
    "frames = vid_data['images']\n",
    "# attributes of interest are \"rep_count\" and \"frame_number\"\n",
    "frame_no = 0\n",
    "# capture frame from video\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "cap.set(1,frame_no) # Where frame_no is the frame you want \n",
    "ret, frame = cap.read() # Read the frame\n",
    "cv2.imshow('window_name', frame) # show frame on window\n",
    "\n",
    "# Wait until user exits\n",
    "while True:\n",
    "    ch = 0xFF & cv2.waitKey(1) # Wait for a second\n",
    "    if ch == 27:\n",
    "        cv2.waitKey(1)\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frame rate of video, is 24 fps\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_df = pd.DataFrame(frames)\n",
    "frames_df['rep_norm'] = frames_df['rep_count'].mod(1)\n",
    "second_state = frames_df[(frames_df['rep_norm'] > 0.45) & (frames_df['rep_norm'] < 0.55)].sort_values('rep_norm')\n",
    "first_state = frames_df[(frames_df['rep_norm'] > 0.95) | (frames_df['rep_norm'] < 0.05)].sort_values('rep_norm')\n",
    "# first_state['frame_numer'].to_list() \n",
    "# code to save image with extension\n",
    "# cv2.imwrite(path+name+'.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of initial exploration:\n",
    "The first terminal state is found in frames labeled with rep counts near k where k is any positive integer.\n",
    "The second terminal state is found in frames labeled with rep counts near k + 0.5 where k is any positive integer. \n",
    "\n",
    "Strategy to pick frames from videos: get frame numbers with rep counts between the ranges of 0.45 and 0.55 for second state and range of (k-1).95 and k.05. From one video there were 45 frames that satisfied this condition.\n",
    "\n",
    "How:\n",
    "- put images into data frame and filter to get frame numbers for each type of state\n",
    "- need to filter to get just the decimal place of the rep count number\n",
    "- select subset of these frames for each state\n",
    "    - random selection of 3 framesor get 2 low vals 2 high vals, 2 middle values\n",
    "- Save frames in the correct output folder\n",
    "\n",
    "make this modular:\n",
    "- function: getFirstState Frames (same for second state)\n",
    "    - input: dataframe, rep_count range\n",
    "    - return: list of frame numbers that satisfy condition\n",
    "- function: save frames\n",
    "    - input: list of frame numbers, video file path\n",
    "    - return: boolean validator\n",
    "\n",
    "\n",
    "Steps:\n",
    "1. Get frame #s for first state and frame #s for second state\n",
    "2. Save frames in the output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap images from sample videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_selection(frame_values, k):\n",
    "    \"\"\"\n",
    "    processes list of frame numbers and returns k choices in the beginning, middle, and end.\n",
    "    \n",
    "    :param frame_values: list of nums \n",
    "    :param k: integer argument\n",
    "    :return: list of 3k choices\n",
    "    \"\"\"\n",
    "    first = frame_values[0:k]\n",
    "    last = frame_values[-k:]\n",
    "    \n",
    "    beg_indx = (len(frame_values) // 2) - (k // 2)\n",
    "    end_indx = (len(frame_values) // 2) + (k // 2)\n",
    "\n",
    "    middle = frame_values[beg_indx:end_indx]\n",
    "\n",
    "    return first + middle + last\n",
    "\n",
    "def save_frames(output_path, video_path, frame_nums, file_name):\n",
    "    \"\"\"\n",
    "    save images of a video using frame numbers\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    for num in frame_nums:\n",
    "        cap.set(1,num)\n",
    "        ret, frame = cap.read() # Read the frame\n",
    "        cv2.imwrite(os.path.join(output_path, file_name+\"_frame_\"+str(num)+\".jpg\"),frame)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping  ARM_RAISE\n",
      "100%|██████████| 74/74 [00:25<00:00,  2.95it/s]\n",
      "Bootstrapping  BICEP_CURL\n",
      "100%|██████████| 70/70 [00:14<00:00,  4.89it/s]\n",
      "Bootstrapping  LEG_RAISE\n",
      "100%|██████████| 42/42 [00:07<00:00,  5.68it/s]\n",
      "Bootstrapping  SQUAT\n",
      "100%|██████████| 62/62 [00:13<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for pose_class in pose_classes:\n",
    "    print(\"Bootstrapping \", pose_class, file=sys.stderr)\n",
    "    file_names = sorted(\n",
    "                [\n",
    "                    n.split('.')[0] \n",
    "                    for n in os.listdir(os.path.join(videos_in_path,pose_class)) \n",
    "                    if ((not n.startswith('.')) and (n.endswith('.json')))\n",
    "                ]\n",
    "    )\n",
    "\n",
    "    # Generate testing data only on 80% of the files available for the pose class\n",
    "    end_idx = round(len(file_names)*0.8)\n",
    "\n",
    "\n",
    "    # Find and save relevant frames for each image file\n",
    "    for file_name in tqdm.tqdm(file_names[0:end_idx], position=0):\n",
    "        data_path = os.path.join(videos_in_path,pose_class,file_name + '.json')\n",
    "        vid_path = os.path.join(videos_in_path,pose_class,file_name + '.mp4')\n",
    "\n",
    "        # Acquire json data \n",
    "        file = open(data_path)\n",
    "        data = json.load(file)\n",
    "        frames = data['images']\n",
    "\n",
    "        # Store data in dataframe\n",
    "        frames_df = pd.DataFrame(frames)\n",
    "        frames_df['rep_norm'] = frames_df['rep_count'].mod(1)\n",
    "\n",
    "        # Filter rep_count param for both first and second state values\n",
    "        second_state = frames_df[(frames_df['rep_norm'] > 0.45) & (frames_df['rep_norm'] < 0.55)].sort_values('rep_norm')\n",
    "        first_state = frames_df[(frames_df['rep_norm'] > 0.95) | (frames_df['rep_norm'] < 0.05)].sort_values('rep_norm')\n",
    "\n",
    "        # Get selection for each terminal state\n",
    "        second_vals = get_frame_selection(second_state['frame_numer'].to_list(), 2)\n",
    "        first_vals = get_frame_selection(first_state['frame_numer'].to_list(), 2)\n",
    "\n",
    "        second_output_path = os.path.join(images_out_folder,pose_class+\"_\"+\"second\")\n",
    "        first_output_path = os.path.join(images_out_folder, pose_class+\"_\"+\"first\")\n",
    "\n",
    "        # Save images for each state\n",
    "        save_frames(second_output_path,vid_path,second_vals, file_name)\n",
    "        save_frames(first_output_path,vid_path,first_vals, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rep Counting Model Development\n",
    "\n",
    "Adapted from Blazepose Pose Classification Solution Documentation\n",
    "Source: https://google.github.io/mediapipe/solutions/pose_classification.html\n",
    "\"We use pairwise distances between predefined lists of pose joints, such as distances between wrist and shoulder, ankle and hip, and two wrists.\"\n",
    "\n",
    "Possible problem: unabole to differentiate between start for squat and arm raise.\n",
    "\n",
    "Possible Strategies to improve model:\n",
    "- Use joint angles instead of pairwise distances, --> then don't need to normalize poses\n",
    "- do some parameter tuning with the smoothing\n",
    "- Get live rep counting from this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show_image(img, figsize=(10, 10)):\n",
    "  \"\"\"Shows output PIL image.\"\"\"\n",
    "  plt.figure(figsize=figsize)\n",
    "  plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullBodyPoseEmbedder(object):\n",
    "  \"\"\"Converts 3D pose landmarks into 3D embedding.\"\"\"\n",
    "\n",
    "  def __init__(self, torso_size_multiplier=2.5):\n",
    "    # Multiplier to apply to the torso to get minimal body size.\n",
    "    self._torso_size_multiplier = torso_size_multiplier\n",
    "\n",
    "    # Names of the landmarks as they appear in the prediction.\n",
    "    self._landmark_names = [\n",
    "        'nose',\n",
    "        'left_eye_inner', 'left_eye', 'left_eye_outer',\n",
    "        'right_eye_inner', 'right_eye', 'right_eye_outer',\n",
    "        'left_ear', 'right_ear',\n",
    "        'mouth_left', 'mouth_right',\n",
    "        'left_shoulder', 'right_shoulder',\n",
    "        'left_elbow', 'right_elbow',\n",
    "        'left_wrist', 'right_wrist',\n",
    "        'left_pinky_1', 'right_pinky_1',\n",
    "        'left_index_1', 'right_index_1',\n",
    "        'left_thumb_2', 'right_thumb_2',\n",
    "        'left_hip', 'right_hip',\n",
    "        'left_knee', 'right_knee',\n",
    "        'left_ankle', 'right_ankle',\n",
    "        'left_heel', 'right_heel',\n",
    "        'left_foot_index', 'right_foot_index',\n",
    "    ]\n",
    "\n",
    "  def __call__(self, landmarks):\n",
    "    \"\"\"Normalizes pose landmarks and converts to embedding\n",
    "    \n",
    "    Args:\n",
    "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Result:\n",
    "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
    "      pairwise distances defined in `_get_pose_distance_embedding`.\n",
    "    \"\"\"\n",
    "    assert landmarks.shape[0] == len(self._landmark_names), 'Unexpected number of landmarks: {}'.format(landmarks.shape[0])\n",
    "\n",
    "    # Get pose landmarks.\n",
    "    landmarks = np.copy(landmarks)\n",
    "\n",
    "    # Normalize landmarks.\n",
    "    landmarks = self._normalize_pose_landmarks(landmarks)\n",
    "\n",
    "    # Get embedding.\n",
    "    embedding = self._get_pose_distance_embedding(landmarks)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "  def _normalize_pose_landmarks(self, landmarks):\n",
    "    \"\"\"Normalizes landmarks translation and scale.\"\"\"\n",
    "    landmarks = np.copy(landmarks)\n",
    "\n",
    "    # Normalize translation.\n",
    "    pose_center = self._get_pose_center(landmarks)\n",
    "    landmarks -= pose_center\n",
    "\n",
    "    # Normalize scale.\n",
    "    pose_size = self._get_pose_size(landmarks, self._torso_size_multiplier)\n",
    "    landmarks /= pose_size\n",
    "    # Multiplication by 100 is not required, but makes it eaasier to debug.\n",
    "    landmarks *= 100\n",
    "\n",
    "    return landmarks\n",
    "\n",
    "  def _get_pose_center(self, landmarks):\n",
    "    \"\"\"Calculates pose center as point between hips.\"\"\"\n",
    "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
    "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
    "    center = (left_hip + right_hip) * 0.5\n",
    "    return center\n",
    "\n",
    "  def _get_pose_size(self, landmarks, torso_size_multiplier):\n",
    "    \"\"\"Calculates pose size.\n",
    "    \n",
    "    It is the maximum of two values:\n",
    "      * Torso size multiplied by `torso_size_multiplier`\n",
    "      * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # This approach uses only 2D landmarks to compute pose size.\n",
    "    landmarks = landmarks[:, :2]\n",
    "\n",
    "    # Hips center.\n",
    "    left_hip = landmarks[self._landmark_names.index('left_hip')]\n",
    "    right_hip = landmarks[self._landmark_names.index('right_hip')]\n",
    "    hips = (left_hip + right_hip) * 0.5\n",
    "\n",
    "    # Shoulders center.\n",
    "    left_shoulder = landmarks[self._landmark_names.index('left_shoulder')]\n",
    "    right_shoulder = landmarks[self._landmark_names.index('right_shoulder')]\n",
    "    shoulders = (left_shoulder + right_shoulder) * 0.5\n",
    "\n",
    "    # Torso size as the minimum body size.\n",
    "    torso_size = np.linalg.norm(shoulders - hips)\n",
    "\n",
    "    # Max dist to pose center.\n",
    "    pose_center = self._get_pose_center(landmarks)\n",
    "    max_dist = np.max(np.linalg.norm(landmarks - pose_center, axis=1))\n",
    "\n",
    "    return max(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "  def _get_pose_distance_embedding(self, landmarks):\n",
    "    \"\"\"Converts pose landmarks into 3D embedding.\n",
    "\n",
    "    We use several pairwise 3D distances to form pose embedding. All distances\n",
    "    include X and Y components with sign. We differnt types of pairs to cover\n",
    "    different pose classes. Feel free to remove some or add new.\n",
    "    \n",
    "    Args:\n",
    "      landmarks - NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Result:\n",
    "      Numpy array with pose embedding of shape (M, 3) where `M` is the number of\n",
    "      pairwise distances.\n",
    "    \"\"\"\n",
    "    embedding = np.array([\n",
    "        # One joint.\n",
    "\n",
    "        self._get_distance(\n",
    "            self._get_average_by_names(landmarks, 'left_hip', 'right_hip'),\n",
    "            self._get_average_by_names(landmarks, 'left_shoulder', 'right_shoulder')),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_elbow'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_elbow'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_elbow', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_elbow', 'right_wrist'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_knee'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_knee'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_knee', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_knee', 'right_ankle'),\n",
    "\n",
    "        # Two joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_wrist'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_ankle'),\n",
    "\n",
    "        # Four joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
    "\n",
    "        # Five joints.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_shoulder', 'left_ankle'),\n",
    "        self._get_distance_by_names(landmarks, 'right_shoulder', 'right_ankle'),\n",
    "        \n",
    "        self._get_distance_by_names(landmarks, 'left_hip', 'left_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'right_hip', 'right_wrist'),\n",
    "\n",
    "        # Cross body.\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_elbow', 'right_elbow'),\n",
    "        self._get_distance_by_names(landmarks, 'left_knee', 'right_knee'),\n",
    "\n",
    "        self._get_distance_by_names(landmarks, 'left_wrist', 'right_wrist'),\n",
    "        self._get_distance_by_names(landmarks, 'left_ankle', 'right_ankle'),\n",
    "\n",
    "        # Body bent direction.\n",
    "\n",
    "        # self._get_distance(\n",
    "        #     self._get_average_by_names(landmarks, 'left_wrist', 'left_ankle'),\n",
    "        #     landmarks[self._landmark_names.index('left_hip')]),\n",
    "        # self._get_distance(\n",
    "        #     self._get_average_by_names(landmarks, 'right_wrist', 'right_ankle'),\n",
    "        #     landmarks[self._landmark_names.index('right_hip')]),\n",
    "    ])\n",
    "\n",
    "    return embedding\n",
    "\n",
    "  def _get_average_by_names(self, landmarks, name_from, name_to):\n",
    "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
    "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
    "    return (lmk_from + lmk_to) * 0.5\n",
    "\n",
    "  def _get_distance_by_names(self, landmarks, name_from, name_to):\n",
    "    lmk_from = landmarks[self._landmark_names.index(name_from)]\n",
    "    lmk_to = landmarks[self._landmark_names.index(name_to)]\n",
    "    return self._get_distance(lmk_from, lmk_to)\n",
    "\n",
    "  def _get_distance(self, lmk_from, lmk_to):\n",
    "    return lmk_to - lmk_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseSample(object):\n",
    "\n",
    "  def __init__(self, name, landmarks, class_name, embedding):\n",
    "    self.name = name\n",
    "    self.landmarks = landmarks\n",
    "    self.class_name = class_name\n",
    "    \n",
    "    self.embedding = embedding\n",
    "\n",
    "\n",
    "class PoseSampleOutlier(object):\n",
    "\n",
    "  def __init__(self, sample, detected_class, all_classes):\n",
    "    self.sample = sample\n",
    "    self.detected_class = detected_class\n",
    "    self.all_classes = all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class PoseClassifier(object):\n",
    "  \"\"\"Classifies pose landmarks.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               pose_samples_folder,\n",
    "               pose_embedder,\n",
    "               file_extension='csv',\n",
    "               file_separator=',',\n",
    "               n_landmarks=33,\n",
    "               n_dimensions=3,\n",
    "               top_n_by_max_distance=30,\n",
    "               top_n_by_mean_distance=10,\n",
    "               axes_weights=(1., 1., 0.2)):\n",
    "    self._pose_embedder = pose_embedder\n",
    "    self._n_landmarks = n_landmarks\n",
    "    self._n_dimensions = n_dimensions\n",
    "    self._top_n_by_max_distance = top_n_by_max_distance\n",
    "    self._top_n_by_mean_distance = top_n_by_mean_distance\n",
    "    self._axes_weights = axes_weights\n",
    "\n",
    "    self._pose_samples = self._load_pose_samples(pose_samples_folder,\n",
    "                                                 file_extension,\n",
    "                                                 file_separator,\n",
    "                                                 n_landmarks,\n",
    "                                                 n_dimensions,\n",
    "                                                 pose_embedder)\n",
    "\n",
    "  def _load_pose_samples(self,\n",
    "                         pose_samples_folder,\n",
    "                         file_extension,\n",
    "                         file_separator,\n",
    "                         n_landmarks,\n",
    "                         n_dimensions,\n",
    "                         pose_embedder):\n",
    "    \"\"\"Loads pose samples from a given folder.\n",
    "    \n",
    "    Required folder structure:\n",
    "      neutral_standing.csv\n",
    "      pushups_down.csv\n",
    "      pushups_up.csv\n",
    "      squats_down.csv\n",
    "      ...\n",
    "\n",
    "    Required CSV structure:\n",
    "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
    "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
    "      ...\n",
    "    \"\"\"\n",
    "    # Each file in the folder represents one pose class.\n",
    "    file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
    "\n",
    "    pose_samples = []\n",
    "    for file_name in file_names:\n",
    "      # Use file name as pose class name.\n",
    "      class_name = file_name[:-(len(file_extension) + 1)]\n",
    "      \n",
    "      # Parse CSV.\n",
    "      with open(os.path.join(pose_samples_folder, file_name)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=file_separator)\n",
    "        for row in csv_reader:\n",
    "          assert len(row) == n_landmarks * n_dimensions + 1, 'Wrong number of values: {}'.format(len(row))\n",
    "          landmarks = np.array(row[1:], np.float32).reshape([n_landmarks, n_dimensions])\n",
    "          pose_samples.append(PoseSample(\n",
    "              name=row[0],\n",
    "              landmarks=landmarks,\n",
    "              class_name=class_name,\n",
    "              embedding=pose_embedder(landmarks),\n",
    "          ))\n",
    "\n",
    "    return pose_samples\n",
    "\n",
    "  def find_pose_sample_outliers(self):\n",
    "    \"\"\"Classifies each sample against the entire database.\"\"\"\n",
    "    # Find outliers in target poses\n",
    "    outliers = []\n",
    "    for sample in self._pose_samples:\n",
    "      # Find nearest poses for the target one.\n",
    "      pose_landmarks = sample.landmarks.copy()\n",
    "      pose_classification = self.__call__(pose_landmarks)\n",
    "      class_names = [class_name for class_name, count in pose_classification.items() if count == max(pose_classification.values())]\n",
    "\n",
    "      # Sample is an outlier if nearest poses have different class or more than\n",
    "      # one pose class is detected as nearest.\n",
    "      if sample.class_name not in class_names or len(class_names) != 1:\n",
    "        outliers.append(PoseSampleOutlier(sample, class_names, pose_classification))\n",
    "\n",
    "    return outliers\n",
    "\n",
    "  def __call__(self, pose_landmarks):\n",
    "    \"\"\"Classifies given pose.\n",
    "\n",
    "    Classification is done in two stages:\n",
    "      * First we pick top-N samples by MAX distance. It allows to remove samples\n",
    "        that are almost the same as given pose, but has few joints bent in the\n",
    "        other direction.\n",
    "      * Then we pick top-N samples by MEAN distance. After outliers are removed\n",
    "        on a previous step, we can pick samples that are closest on average.\n",
    "    \n",
    "    Args:\n",
    "      pose_landmarks: NumPy array with 3D landmarks of shape (N, 3).\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with count of nearest pose samples from the database. Sample:\n",
    "        {\n",
    "          'pushups_down': 8,\n",
    "          'pushups_up': 2,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Check that provided and target poses have the same shape.\n",
    "    assert pose_landmarks.shape == (self._n_landmarks, self._n_dimensions), 'Unexpected shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "    # Get given pose embedding.\n",
    "    pose_embedding = self._pose_embedder(pose_landmarks)\n",
    "    flipped_pose_embedding = self._pose_embedder(pose_landmarks * np.array([-1, 1, 1]))\n",
    "\n",
    "    # Filter by max distance.\n",
    "    #\n",
    "    # That helps to remove outliers - poses that are almost the same as the\n",
    "    # given one, but has one joint bent into another direction and actually\n",
    "    # represnt a different pose class.\n",
    "    max_dist_heap = []\n",
    "    for sample_idx, sample in enumerate(self._pose_samples):\n",
    "      max_dist = min(\n",
    "          np.max(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.max(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      max_dist_heap.append([max_dist, sample_idx])\n",
    "\n",
    "    max_dist_heap = sorted(max_dist_heap, key=lambda x: x[0])\n",
    "    max_dist_heap = max_dist_heap[:self._top_n_by_max_distance]\n",
    "\n",
    "    # Filter by mean distance.\n",
    "    #\n",
    "    # After removing outliers we can find the nearest pose by mean distance.\n",
    "    mean_dist_heap = []\n",
    "    for _, sample_idx in max_dist_heap:\n",
    "      sample = self._pose_samples[sample_idx]\n",
    "      mean_dist = min(\n",
    "          np.mean(np.abs(sample.embedding - pose_embedding) * self._axes_weights),\n",
    "          np.mean(np.abs(sample.embedding - flipped_pose_embedding) * self._axes_weights),\n",
    "      )\n",
    "      mean_dist_heap.append([mean_dist, sample_idx])\n",
    "\n",
    "    mean_dist_heap = sorted(mean_dist_heap, key=lambda x: x[0])\n",
    "    mean_dist_heap = mean_dist_heap[:self._top_n_by_mean_distance]\n",
    "\n",
    "    # Collect results into map: (class_name -> n_samples)\n",
    "    class_names = [self._pose_samples[sample_idx].class_name for _, sample_idx in mean_dist_heap]\n",
    "    result = {class_name: class_names.count(class_name) for class_name in set(class_names)}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMADictSmoothing(object):\n",
    "  \"\"\"Smoothes pose classification.\"\"\"\n",
    "\n",
    "  def __init__(self, window_size=10, alpha=0.2):\n",
    "    self._window_size = window_size\n",
    "    self._alpha = alpha\n",
    "\n",
    "    self._data_in_window = []\n",
    "\n",
    "  def __call__(self, data):\n",
    "    \"\"\"Smoothes given pose classification.\n",
    "\n",
    "    Smoothing is done by computing Exponential Moving Average for every pose\n",
    "    class observed in the given time window. Missed pose classes arre replaced\n",
    "    with 0.\n",
    "    \n",
    "    Args:\n",
    "      data: Dictionary with pose classification. Sample:\n",
    "          {\n",
    "            'pushups_down': 8,\n",
    "            'pushups_up': 2,\n",
    "          }\n",
    "\n",
    "    Result:\n",
    "      Dictionary in the same format but with smoothed and float instead of\n",
    "      integer values. Sample:\n",
    "        {\n",
    "          'pushups_down': 8.3,\n",
    "          'pushups_up': 1.7,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Add new data to the beginning of the window for simpler code.\n",
    "    self._data_in_window.insert(0, data)\n",
    "    self._data_in_window = self._data_in_window[:self._window_size]\n",
    "\n",
    "    # Get all keys.\n",
    "    keys = set([key for data in self._data_in_window for key, _ in data.items()])\n",
    "\n",
    "    # Get smoothed values.\n",
    "    smoothed_data = dict()\n",
    "    for key in keys:\n",
    "      factor = 1.0\n",
    "      top_sum = 0.0\n",
    "      bottom_sum = 0.0\n",
    "      for data in self._data_in_window:\n",
    "        value = data[key] if key in data else 0.0\n",
    "\n",
    "        top_sum += factor * value\n",
    "        bottom_sum += factor\n",
    "\n",
    "        # Update factor.\n",
    "        factor *= (1.0 - self._alpha)\n",
    "\n",
    "      smoothed_data[key] = top_sum / bottom_sum\n",
    "\n",
    "    return smoothed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repitition Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepetitionCounter(object):\n",
    "  \"\"\"Counts number of repetitions of given target pose class.\"\"\"\n",
    "\n",
    "  def __init__(self, class_name, enter_threshold=6, exit_threshold=4):\n",
    "    self._class_name = class_name\n",
    "\n",
    "    # If pose counter passes given threshold, then we enter the pose.\n",
    "    self._enter_threshold = enter_threshold\n",
    "    self._exit_threshold = exit_threshold\n",
    "\n",
    "    # Either we are in given pose or not.\n",
    "    self._pose_entered = False\n",
    "\n",
    "    # Number of times we exited the pose.\n",
    "    self._n_repeats = 0\n",
    "\n",
    "  @property\n",
    "  def n_repeats(self):\n",
    "    return self._n_repeats\n",
    "\n",
    "  def __call__(self, pose_classification):\n",
    "    \"\"\"Counts number of repetitions happend until given frame.\n",
    "\n",
    "    We use two thresholds. First you need to go above the higher one to enter\n",
    "    the pose, and then you need to go below the lower one to exit it. Difference\n",
    "    between the thresholds makes it stable to prediction jittering (which will\n",
    "    cause wrong counts in case of having only one threshold).\n",
    "    \n",
    "    Args:\n",
    "      pose_classification: Pose classification dictionary on current frame.\n",
    "        Sample:\n",
    "          {\n",
    "            'pushups_down': 8.3,\n",
    "            'pushups_up': 1.7,\n",
    "          }\n",
    "\n",
    "    Returns:\n",
    "      Integer counter of repetitions.\n",
    "    \"\"\"\n",
    "    # Get pose confidence.\n",
    "    pose_confidence = 0.0\n",
    "    if self._class_name in pose_classification:\n",
    "      pose_confidence = pose_classification[self._class_name]\n",
    "\n",
    "    # On the very first frame or if we were out of the pose, just check if we\n",
    "    # entered it on this frame and update the state.\n",
    "    if not self._pose_entered:\n",
    "      self._pose_entered = pose_confidence > self._enter_threshold\n",
    "      return self._n_repeats\n",
    "\n",
    "    # If we were in the pose and are exiting it, then increase the counter and\n",
    "    # update the state.\n",
    "    if pose_confidence < self._exit_threshold:\n",
    "      self._n_repeats += 1\n",
    "      self._pose_entered = False\n",
    "\n",
    "    return self._n_repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "import requests\n",
    "\n",
    "class PoseClassificationVisualizer(object):\n",
    "  \"\"\"Keeps track of claassifcations for every frame and renders them.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               class_name,\n",
    "               plot_location_x=0.05,\n",
    "               plot_location_y=0.05,\n",
    "               plot_max_width=0.4,\n",
    "               plot_max_height=0.4,\n",
    "               plot_figsize=(9, 4),\n",
    "               plot_x_max=None,\n",
    "               plot_y_max=None,\n",
    "               counter_location_x=0.85,\n",
    "               counter_location_y=0.05,\n",
    "               counter_font_path='https://github.com/googlefonts/roboto/blob/main/src/hinted/Roboto-Regular.ttf?raw=true',\n",
    "               counter_font_color='red',\n",
    "               counter_font_size=0.15):\n",
    "    self._class_name = class_name\n",
    "    self._plot_location_x = plot_location_x\n",
    "    self._plot_location_y = plot_location_y\n",
    "    self._plot_max_width = plot_max_width\n",
    "    self._plot_max_height = plot_max_height\n",
    "    self._plot_figsize = plot_figsize\n",
    "    self._plot_x_max = plot_x_max\n",
    "    self._plot_y_max = plot_y_max\n",
    "    self._counter_location_x = counter_location_x\n",
    "    self._counter_location_y = counter_location_y\n",
    "    self._counter_font_path = counter_font_path\n",
    "    self._counter_font_color = counter_font_color\n",
    "    self._counter_font_size = counter_font_size\n",
    "\n",
    "    self._counter_font = None\n",
    "\n",
    "    self._pose_classification_history = []\n",
    "    self._pose_classification_filtered_history = []\n",
    "\n",
    "  def __call__(self,\n",
    "               frame,\n",
    "               pose_classification,\n",
    "               pose_classification_filtered,\n",
    "               repetitions_count):\n",
    "    \"\"\"Renders pose classifcation and counter until given frame.\"\"\"\n",
    "    # Extend classification history.\n",
    "    self._pose_classification_history.append(pose_classification)\n",
    "    self._pose_classification_filtered_history.append(pose_classification_filtered)\n",
    "\n",
    "    # Output frame with classification plot and counter.\n",
    "    output_img = Image.fromarray(frame)\n",
    "\n",
    "    output_width = output_img.size[0]\n",
    "    output_height = output_img.size[1]\n",
    "\n",
    "    # Draw the plot.\n",
    "    img = self._plot_classification_history(output_width, output_height)\n",
    "    img.thumbnail((int(output_width * self._plot_max_width),\n",
    "                   int(output_height * self._plot_max_height)),\n",
    "                  Image.ANTIALIAS)\n",
    "    output_img.paste(img,\n",
    "                     (int(output_width * self._plot_location_x),\n",
    "                      int(output_height * self._plot_location_y)))\n",
    "\n",
    "    # Draw the count.\n",
    "    output_img_draw = ImageDraw.Draw(output_img)\n",
    "    if self._counter_font is None:\n",
    "      font_size = int(output_height * self._counter_font_size)\n",
    "      font_request = requests.get(self._counter_font_path, allow_redirects=True)\n",
    "      self._counter_font = ImageFont.truetype(io.BytesIO(font_request.content), size=font_size)\n",
    "    output_img_draw.text((output_width * self._counter_location_x,\n",
    "                          output_height * self._counter_location_y),\n",
    "                         str(repetitions_count),\n",
    "                         font=self._counter_font,\n",
    "                         fill=self._counter_font_color)\n",
    "\n",
    "    return output_img\n",
    "\n",
    "  def _plot_classification_history(self, output_width, output_height):\n",
    "    fig = plt.figure(figsize=self._plot_figsize)\n",
    "\n",
    "    for classification_history in [self._pose_classification_history,\n",
    "                                   self._pose_classification_filtered_history]:\n",
    "      y = []\n",
    "      for classification in classification_history:\n",
    "        if classification is None:\n",
    "          y.append(None)\n",
    "        elif self._class_name in classification:\n",
    "          y.append(classification[self._class_name])\n",
    "        else:\n",
    "          y.append(0)\n",
    "      plt.plot(y, linewidth=7)\n",
    "\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Confidence')\n",
    "    plt.title('Classification history for `{}`'.format(self._class_name))\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    if self._plot_y_max is not None:\n",
    "      plt.ylim(top=self._plot_y_max)\n",
    "    if self._plot_x_max is not None:\n",
    "      plt.xlim(right=self._plot_x_max)\n",
    "\n",
    "    # Convert plot to image.\n",
    "    buf = io.BytesIO()\n",
    "    dpi = min(\n",
    "        output_width * self._plot_max_width / float(self._plot_figsize[0]),\n",
    "        output_height * self._plot_max_height / float(self._plot_figsize[1]))\n",
    "    fig.savefig(buf, dpi=dpi)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "\n",
    "class BootstrapHelper(object):\n",
    "  \"\"\"Helps to bootstrap images and filter pose samples for classification.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               images_in_folder,\n",
    "               images_out_folder,\n",
    "               csvs_out_folder):\n",
    "    self._images_in_folder = images_in_folder\n",
    "    self._images_out_folder = images_out_folder\n",
    "    self._csvs_out_folder = csvs_out_folder\n",
    "\n",
    "    # Get list of pose classes and print image statistics.\n",
    "    self._pose_class_names = sorted([n for n in os.listdir(self._images_in_folder) if not n.startswith('.')])\n",
    "    \n",
    "  def bootstrap(self, per_pose_class_limit=None):\n",
    "    \"\"\"Bootstraps images in a given folder.\n",
    "    \n",
    "    Required image in folder (same use for image out folder):\n",
    "      pushups_up/\n",
    "        image_001.jpg\n",
    "        image_002.jpg\n",
    "        ...\n",
    "      pushups_down/\n",
    "        image_001.jpg\n",
    "        image_002.jpg\n",
    "        ...\n",
    "      ...\n",
    "\n",
    "    Produced CSVs out folder:\n",
    "      pushups_up.csv\n",
    "      pushups_down.csv\n",
    "\n",
    "    Produced CSV structure with pose 3D landmarks:\n",
    "      sample_00001,x1,y1,z1,x2,y2,z2,....\n",
    "      sample_00002,x1,y1,z1,x2,y2,z2,....\n",
    "    \"\"\"\n",
    "    # Create output folder for CVSs.\n",
    "    if not os.path.exists(self._csvs_out_folder):\n",
    "      os.makedirs(self._csvs_out_folder)\n",
    "\n",
    "    for pose_class_name in self._pose_class_names:\n",
    "      print('Bootstrapping ', pose_class_name, file=sys.stderr)\n",
    "\n",
    "      # Paths for the pose class.\n",
    "      images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
    "      csv_out_path = os.path.join(self._csvs_out_folder, pose_class_name + '.csv')\n",
    "      if not os.path.exists(images_out_folder):\n",
    "        os.makedirs(images_out_folder)\n",
    "\n",
    "      with open(csv_out_path, 'w') as csv_out_file:\n",
    "        csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        # Get list of images.\n",
    "        image_names = sorted([n for n in os.listdir(images_in_folder) if not n.startswith('.')])\n",
    "        if per_pose_class_limit is not None:\n",
    "          image_names = image_names[:per_pose_class_limit]\n",
    "\n",
    "        # Bootstrap every image.\n",
    "        for image_name in tqdm.tqdm(image_names):\n",
    "          # Load image.\n",
    "          input_frame = cv2.imread(os.path.join(images_in_folder, image_name))\n",
    "          input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "          # Initialize fresh pose tracker and run it.\n",
    "          with mp_pose.Pose(\n",
    "                static_image_mode=True,\n",
    "                min_detection_confidence=0.5,\n",
    "                min_tracking_confidence=0.5,\n",
    "                model_complexity=2,\n",
    "            ) as pose_tracker:\n",
    "            result = pose_tracker.process(image=input_frame)\n",
    "            pose_landmarks = result.pose_landmarks\n",
    "\n",
    "          # Save image with pose prediction (if pose was detected).\n",
    "          output_frame = input_frame.copy()\n",
    "          if pose_landmarks is not None:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=output_frame,\n",
    "                landmark_list=pose_landmarks,\n",
    "                connections=mp_pose.POSE_CONNECTIONS)\n",
    "          output_frame = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
    "          cv2.imwrite(os.path.join(images_out_folder, image_name), output_frame)\n",
    "          \n",
    "          # Save landmarks if pose was detected.\n",
    "          if pose_landmarks is not None:\n",
    "            # Get landmarks.\n",
    "            frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "            pose_landmarks = np.array(\n",
    "                [[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                 for lmk in pose_landmarks.landmark],\n",
    "                dtype=np.float32)\n",
    "            assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "            csv_out_writer.writerow([image_name] + pose_landmarks.flatten().astype(np.str).tolist())\n",
    "\n",
    "          # Draw XZ projection and concatenate with the image.\n",
    "          projection_xz = self._draw_xz_projection(\n",
    "              output_frame=output_frame, pose_landmarks=pose_landmarks)\n",
    "          output_frame = np.concatenate((output_frame, projection_xz), axis=1)\n",
    "\n",
    "  def _draw_xz_projection(self, output_frame, pose_landmarks, r=0.5, color='red'):\n",
    "    frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "    img = Image.new('RGB', (frame_width, frame_height), color='white')\n",
    "\n",
    "    if pose_landmarks is None:\n",
    "      return np.asarray(img)\n",
    "\n",
    "    # Scale radius according to the image width.\n",
    "    r *= frame_width * 0.01\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for idx_1, idx_2 in mp_pose.POSE_CONNECTIONS:\n",
    "      # Flip Z and move hips center to the center of the image.\n",
    "      x1, y1, z1 = pose_landmarks[idx_1] * [1, 1, -1] + [0, 0, frame_height * 0.5]\n",
    "      x2, y2, z2 = pose_landmarks[idx_2] * [1, 1, -1] + [0, 0, frame_height * 0.5]\n",
    "\n",
    "      draw.ellipse([x1 - r, z1 - r, x1 + r, z1 + r], fill=color)\n",
    "      draw.ellipse([x2 - r, z2 - r, x2 + r, z2 + r], fill=color)\n",
    "      draw.line([x1, z1, x2, z2], width=int(r), fill=color)\n",
    "\n",
    "    return np.asarray(img)\n",
    "\n",
    "  def align_images_and_csvs(self, print_removed_items=False):\n",
    "    \"\"\"Makes sure that image folders and CSVs have the same sample.\n",
    "    \n",
    "    Leaves only intersetion of samples in both image folders and CSVs.\n",
    "    \"\"\"\n",
    "    for pose_class_name in self._pose_class_names:\n",
    "      # Paths for the pose class.\n",
    "      images_out_folder = os.path.join(self._images_out_folder, pose_class_name)\n",
    "      csv_out_path = os.path.join(self._csvs_out_folder, pose_class_name + '.csv')\n",
    "\n",
    "      # Read CSV into memory.\n",
    "      rows = []\n",
    "      with open(csv_out_path) as csv_out_file:\n",
    "        csv_out_reader = csv.reader(csv_out_file, delimiter=',')\n",
    "        for row in csv_out_reader:\n",
    "          rows.append(row)\n",
    "\n",
    "      # Image names left in CSV.\n",
    "      image_names_in_csv = []\n",
    "\n",
    "      # Re-write the CSV removing lines without corresponding images.\n",
    "      with open(csv_out_path, 'w') as csv_out_file:\n",
    "        csv_out_writer = csv.writer(csv_out_file, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in rows:\n",
    "          image_name = row[0]\n",
    "          image_path = os.path.join(images_out_folder, image_name)\n",
    "          if os.path.exists(image_path):\n",
    "            image_names_in_csv.append(image_name)\n",
    "            csv_out_writer.writerow(row)\n",
    "          elif print_removed_items:\n",
    "            print('Removed image from CSV: ', image_path)\n",
    "\n",
    "      # Remove images without corresponding line in CSV.\n",
    "      for image_name in os.listdir(images_out_folder):\n",
    "        if image_name not in image_names_in_csv:\n",
    "          image_path = os.path.join(images_out_folder, image_name)\n",
    "          os.remove(image_path)\n",
    "          if print_removed_items:\n",
    "            print('Removed image from folder: ', image_path)\n",
    "\n",
    "  def analyze_outliers(self, outliers):\n",
    "    \"\"\"Classifies each sample agains all other to find outliers.\n",
    "    \n",
    "    If sample is classified differrrently than the original class - it sould\n",
    "    either be deleted or more similar samples should be added.\n",
    "    \"\"\"\n",
    "    for outlier in outliers:\n",
    "      image_path = os.path.join(self._images_out_folder, outlier.sample.class_name, outlier.sample.name)\n",
    "\n",
    "      print('Outlier')\n",
    "      print('  sample path =    ', image_path)\n",
    "      print('  sample class =   ', outlier.sample.class_name)\n",
    "      print('  detected class = ', outlier.detected_class)\n",
    "      print('  all classes =    ', outlier.all_classes)\n",
    "\n",
    "      img = cv2.imread(image_path)\n",
    "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "      show_image(img, figsize=(20, 20))\n",
    "\n",
    "  def remove_outliers(self, outliers):\n",
    "    \"\"\"Removes outliers from the image folders.\"\"\"\n",
    "    for outlier in outliers:\n",
    "      image_path = os.path.join(self._images_out_folder, outlier.sample.class_name, outlier.sample.name)\n",
    "      os.remove(image_path)\n",
    "\n",
    "  def print_images_in_statistics(self):\n",
    "    \"\"\"Prints statistics from the input image folder.\"\"\"\n",
    "    self._print_images_statistics(self._images_in_folder, self._pose_class_names)\n",
    "\n",
    "  def print_images_out_statistics(self):\n",
    "    \"\"\"Prints statistics from the output image folder.\"\"\"\n",
    "    self._print_images_statistics(self._images_out_folder, self._pose_class_names)\n",
    "\n",
    "  def _print_images_statistics(self, images_folder, pose_class_names):\n",
    "    print('Number of images per pose class:')\n",
    "    for pose_class_name in pose_class_names:\n",
    "      n_images = len([\n",
    "          n for n in os.listdir(os.path.join(images_folder, pose_class_name))\n",
    "          if not n.startswith('.')])\n",
    "      print('  {}: {}'.format(pose_class_name, n_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required structure of the images_in_folder:\n",
    "#\n",
    "#   fitness_poses_images_in/\n",
    "#     pushups_up/\n",
    "#       image_001.jpg\n",
    "#       image_002.jpg\n",
    "#       ...\n",
    "#     pushups_down/\n",
    "#       image_001.jpg\n",
    "#       image_002.jpg\n",
    "#       ...\n",
    "#     ...\n",
    "bootstrap_images_in_folder = 'rep_count_images'\n",
    "\n",
    "# Output folders for bootstrapped images and CSVs.\n",
    "bootstrap_images_out_folder = 'fitness_poses_images_out'\n",
    "bootstrap_csvs_out_folder = 'fitness_poses_csvs_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize helper.\n",
    "bootstrap_helper = BootstrapHelper(\n",
    "    images_in_folder=bootstrap_images_in_folder,\n",
    "    images_out_folder=bootstrap_images_out_folder,\n",
    "    csvs_out_folder=bootstrap_csvs_out_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  ARM_RAISE_first: 444\n",
      "  ARM_RAISE_second: 444\n",
      "  BICEP_CURL_first: 420\n",
      "  BICEP_CURL_second: 420\n",
      "  LEG_RAISE_first: 252\n",
      "  LEG_RAISE_second: 252\n",
      "  SQUAT_first: 372\n",
      "  SQUAT_second: 372\n"
     ]
    }
   ],
   "source": [
    "# Check how many pose classes and images for them are available.\n",
    "bootstrap_helper.print_images_in_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bootstrapping  ARM_RAISE_first\n",
      "  0%|          | 0/444 [00:00<?, ?it/s]INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "/var/folders/4d/0_lvmgpj3z94tvt7zl8k29vw0000gn/T/ipykernel_49564/2136786563.py:105: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  csv_out_writer.writerow([image_name] + pose_landmarks.flatten().astype(np.str).tolist())\n",
      "100%|██████████| 444/444 [03:28<00:00,  2.13it/s]\n",
      "Bootstrapping  ARM_RAISE_second\n",
      "100%|██████████| 444/444 [03:58<00:00,  1.86it/s]\n",
      "Bootstrapping  BICEP_CURL_first\n",
      "100%|██████████| 420/420 [03:22<00:00,  2.07it/s]\n",
      "Bootstrapping  BICEP_CURL_second\n",
      "100%|██████████| 420/420 [03:17<00:00,  2.13it/s]\n",
      "Bootstrapping  LEG_RAISE_first\n",
      "100%|██████████| 252/252 [01:55<00:00,  2.17it/s]\n",
      "Bootstrapping  LEG_RAISE_second\n",
      "100%|██████████| 252/252 [01:55<00:00,  2.18it/s]\n",
      "Bootstrapping  SQUAT_first\n",
      "100%|██████████| 372/372 [02:56<00:00,  2.11it/s]\n",
      "Bootstrapping  SQUAT_second\n",
      "100%|██████████| 372/372 [03:00<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap all images.\n",
    "# Set limit to some small number for debug.\n",
    "bootstrap_helper.bootstrap(per_pose_class_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  ARM_RAISE_first: 444\n",
      "  ARM_RAISE_second: 444\n",
      "  BICEP_CURL_first: 420\n",
      "  BICEP_CURL_second: 420\n",
      "  LEG_RAISE_first: 252\n",
      "  LEG_RAISE_second: 252\n",
      "  SQUAT_first: 372\n",
      "  SQUAT_second: 372\n"
     ]
    }
   ],
   "source": [
    "# Check how many images were bootstrapped.\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  ARM_RAISE_first: 439\n",
      "  ARM_RAISE_second: 410\n",
      "  BICEP_CURL_first: 417\n",
      "  BICEP_CURL_second: 410\n",
      "  LEG_RAISE_first: 132\n",
      "  LEG_RAISE_second: 176\n",
      "  SQUAT_first: 368\n",
      "  SQUAT_second: 358\n"
     ]
    }
   ],
   "source": [
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers:  87\n"
     ]
    }
   ],
   "source": [
    "# Find outliers.\n",
    "\n",
    "# Transforms pose landmarks into embedding.\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# Classifies give pose against database of poses.\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=bootstrap_csvs_out_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "outliers = pose_classifier.find_pose_sample_outliers()\n",
    "print('Number of outliers: ', len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all outliers (if you don't want to manually pick).\n",
    "bootstrap_helper.remove_outliers(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images per pose class:\n",
      "  ARM_RAISE_first: 430\n",
      "  ARM_RAISE_second: 397\n",
      "  BICEP_CURL_first: 409\n",
      "  BICEP_CURL_second: 405\n",
      "  LEG_RAISE_first: 126\n",
      "  LEG_RAISE_second: 156\n",
      "  SQUAT_first: 351\n",
      "  SQUAT_second: 349\n"
     ]
    }
   ],
   "source": [
    "# Align CSVs with images after removing outliers.\n",
    "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
    "bootstrap_helper.print_images_out_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Classifiation on a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your video name and target pose class to count the repetitions.\n",
    "video_path = './personal_data/videos/squat/sample2.m4v'\n",
    "class_name='SQUAT_second'\n",
    "out_video_path = './test_rep_video.m4v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video.\n",
    "import cv2\n",
    "\n",
    "video_cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get some video parameters to generate output video with classificaiton.\n",
    "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize tracker, classifier and counter.\n",
    "# Do that before every video as all of them have state.\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "\n",
    "# Folder with pose class CSVs. That should be the same folder you using while\n",
    "# building classifier to output CSVs.\n",
    "pose_samples_folder = 'fitness_poses_csvs_out'\n",
    "\n",
    "# Initialize tracker.\n",
    "pose_tracker = mp_pose.Pose(\n",
    "                min_detection_confidence=0.5,\n",
    "                min_tracking_confidence=0.5,\n",
    "                model_complexity=2,\n",
    "            )\n",
    "\n",
    "# Initialize embedder.\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# Initialize classifier.\n",
    "# Ceck that you are using the same parameters as during bootstrapping.\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=pose_samples_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "# # Uncomment to validate target poses used by classifier and find outliers.\n",
    "# outliers = pose_classifier.find_pose_sample_outliers()\n",
    "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
    "\n",
    "# Initialize EMA smoothing.\n",
    "pose_classification_filter = EMADictSmoothing(\n",
    "    window_size=10,\n",
    "    alpha=0.2)\n",
    "\n",
    "# Initialize counter.\n",
    "repetition_counter = RepetitionCounter(\n",
    "    class_name=class_name,\n",
    "    enter_threshold=6,\n",
    "    exit_threshold=4)\n",
    "\n",
    "# Initialize renderer.\n",
    "pose_classification_visualizer = PoseClassificationVisualizer(\n",
    "    class_name=class_name,\n",
    "    plot_x_max=video_n_frames,\n",
    "    # Graphic looks nicer if it's the same as `top_n_by_mean_distance`.\n",
    "    plot_y_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification on a video.\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "\n",
    "# Open output video.\n",
    "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "while True:\n",
    "  # Get next frame of the video.\n",
    "  success, input_frame = video_cap.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  # Run pose tracker.\n",
    "  input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "  result = pose_tracker.process(image=input_frame)\n",
    "  pose_landmarks = result.pose_landmarks\n",
    "\n",
    "  # Draw pose prediction.\n",
    "  output_frame = input_frame.copy()\n",
    "  if pose_landmarks is not None:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=output_frame,\n",
    "        landmark_list=pose_landmarks,\n",
    "        connections=mp_pose.POSE_CONNECTIONS)\n",
    "  \n",
    "  if pose_landmarks is not None:\n",
    "    # Get landmarks.\n",
    "    frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "    pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "    assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "    # Classify the pose on the current frame.\n",
    "    pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "    # Smooth classification using EMA.\n",
    "    pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "    # Count repetitions.\n",
    "    repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "  else:\n",
    "    # No pose => no classification on current frame.\n",
    "    pose_classification = None\n",
    "\n",
    "    # Still add empty classification to the filter to maintaing correct\n",
    "    # smoothing for future frames.\n",
    "    pose_classification_filtered = pose_classification_filter(dict())\n",
    "    pose_classification_filtered = None\n",
    "\n",
    "    # Don't update the counter presuming that person is 'frozen'. Just\n",
    "    # take the latest repetitions count.\n",
    "    repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "  # Draw classification plot and repetition counter.\n",
    "  output_frame = pose_classification_visualizer(\n",
    "      frame=output_frame,\n",
    "      pose_classification=pose_classification,\n",
    "      pose_classification_filtered=pose_classification_filtered,\n",
    "      repetitions_count=repetitions_count)\n",
    "\n",
    "  # Save the output frame.\n",
    "  out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "  # # Show intermediate frames of the video to track progress.\n",
    "  # if frame_idx % 50 == 0:\n",
    "  #   show_image(output_frame)\n",
    "\n",
    "  frame_idx += 1\n",
    "\n",
    "# Close output video.\n",
    "out_video.release()\n",
    "\n",
    "# Release MediaPipe resources.\n",
    "pose_tracker.close()\n",
    "\n",
    "# Show the last frame of the video.\n",
    "if output_frame is not None:\n",
    "  show_image(output_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Classifiation on Live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier and counter.\n",
    "class_name='SQUAT_second'\n",
    "\n",
    "# Folder with pose class CSVs. That should be the same folder you using while\n",
    "# building classifier to output CSVs.\n",
    "pose_samples_folder = 'fitness_poses_csvs_out'\n",
    "\n",
    "# Initialize tracker.\n",
    "pose_tracker = mp_pose.Pose(\n",
    "                min_detection_confidence=0.5,\n",
    "                min_tracking_confidence=0.5,\n",
    "                model_complexity=1,\n",
    "            )\n",
    "\n",
    "# Initialize embedder.\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# Initialize classifier.\n",
    "# Ceck that you are using the same parameters as during bootstrapping.\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=pose_samples_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "# # Uncomment to validate target poses used by classifier and find outliers.\n",
    "# outliers = pose_classifier.find_pose_sample_outliers()\n",
    "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
    "\n",
    "# Initialize EMA smoothing.\n",
    "pose_classification_filter = EMADictSmoothing(\n",
    "    window_size=10,\n",
    "    alpha=0.2)\n",
    "\n",
    "# Initialize counter.\n",
    "repetition_counter = RepetitionCounter(\n",
    "    class_name=class_name,\n",
    "    enter_threshold=6,\n",
    "    exit_threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 1280, Height: 720\n",
      "FPS: 1.0\n",
      "Average Iteration Time: 0.15674203300902775\n"
     ]
    }
   ],
   "source": [
    "# Run classification on a video.\n",
    "import os\n",
    "import time\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# For saving video test\n",
    "name = \"./personal_data/rep_counter_test3.m4v\"\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(name, fourcc, 4.0, (1280, 720))\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "class_name='SQUAT_second'\n",
    "\n",
    "# Open video\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "\n",
    "# https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "if not video_cap.isOpened():\n",
    "  print(\"Cannot open camera\")\n",
    "  exit()\n",
    "\n",
    "\n",
    "# Get some video parameters to generate output video with classificaiton.\n",
    "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "print(f\"Width: {video_width}, Height: {video_height}\")\n",
    "print(f\"FPS: {video_fps}\")\n",
    "\n",
    "# FREQ needs to be between 1 and FPS value\n",
    "# FREQ determines how many times per second, the classification model\n",
    "# runs\n",
    "FREQ = 30\n",
    "PERIOD = int(max(1,30/FREQ))\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "# track loop iteration times\n",
    "durations = []\n",
    "\n",
    "while video_cap.isOpened():\n",
    "  # Get next frame of the video.\n",
    "  start = time.time()\n",
    "\n",
    "  success, input_frame = video_cap.read()\n",
    "  if not success:\n",
    "    break\n",
    "\n",
    "  if(input_frame is None):\n",
    "    assert(\"Bad image\")\n",
    "\n",
    "  # Run pose tracker.\n",
    "  input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "  result = pose_tracker.process(image=input_frame)\n",
    "  pose_landmarks = result.pose_landmarks\n",
    "\n",
    "\n",
    "  pose_time = time.time()\n",
    "\n",
    "  # Draw pose prediction.\n",
    "  output_frame = cv2.cvtColor(input_frame.copy(), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "  if pose_landmarks is not None:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=output_frame,\n",
    "        landmark_list=pose_landmarks,\n",
    "        connections=mp_pose.POSE_CONNECTIONS)\n",
    "  \n",
    "  # Determine if we run the classification in this frame\n",
    "  flag = frame_idx % PERIOD == 0\n",
    "  if flag:\n",
    "    if pose_landmarks is not None:\n",
    "      # Get landmarks.\n",
    "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                  for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "      # Classify the pose on the current frame.\n",
    "      pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "      # Smooth classification using EMA.\n",
    "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "      # Count repetitions.\n",
    "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "    else:\n",
    "      # No pose => no classification on current frame.\n",
    "      pose_classification = None\n",
    "\n",
    "      # Still add empty classification to the filter to maintaing correct\n",
    "      # smoothing for future frames.\n",
    "      pose_classification_filtered = pose_classification_filter(dict())\n",
    "      pose_classification_filtered = None\n",
    "\n",
    "      # Don't update the counter presuming that person is 'frozen'. Just\n",
    "      # take the latest repetitions count.\n",
    "      repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "  # # Draw classification plot and repetition counter.\n",
    "  # output_frame = pose_classification_visualizer(\n",
    "  #     frame=output_frame,\n",
    "  #     pose_classification=pose_classification,\n",
    "  #     pose_classification_filtered=pose_classification_filtered,\n",
    "  #     repetitions_count=repetitions_count)\n",
    "\n",
    "  # Add rep counter on the side\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "  cv2.putText(\n",
    "      output_frame, str(repetitions_count), (200, 250), font, 7, (0, 255, 255), 4, cv2.LINE_AA\n",
    "  )\n",
    "\n",
    "  # Display count to user\n",
    "  cv2.imshow('MediaPipe Pose', output_frame)\n",
    "  \n",
    "  # write the flipped frame\n",
    "  out.write(output_frame)\n",
    "\n",
    "\n",
    "  frame_idx += 1\n",
    "\n",
    "  k = cv2.waitKey(1)\n",
    "\n",
    "\n",
    "  if k == ord('q'):\n",
    "    break\n",
    "  \n",
    "  stop = time.time()\n",
    "  pose_proc = pose_time - start\n",
    "  rep_proc = stop-pose_time\n",
    "  durations.append(stop-start)\n",
    "\n",
    "\n",
    "# Release MediaPipe resources.\n",
    "pose_tracker.close()\n",
    "\n",
    "video_cap.release()\n",
    "out.release()\n",
    "\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "print(f\"Average Iteration Time: {np.mean(durations)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_x86_man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcf522a34ca6f641e179087f46089808923a2e6563b163ecccaafb2aae08e069"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
